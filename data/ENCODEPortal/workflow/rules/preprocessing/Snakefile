import yaml 
endtype=["pairedend", "singleend"]
ASSAYS=["DHS", "H3K27ac", "ATAC"]

config = yaml.load(open('../../envs/wd.yaml'))

# obtain PAIREDEND SAMPLES
with open(config['params_preprocess']['preprocess_dir']+"unique_pairedend_h3k27ac_dhs_files.tsv", "r") as f:
        PAIREDEND_SAMPLES = [line.split(".")[0] for line in f.readlines()]
f.close()
#print(PAIREDEND_SAMPLES)

# obtain SINGLEEND SAMPLES
with open(config['params_preprocess']['preprocess_dir']+"unique_singleend_h3k27ac_dhs_files.tsv", "r") as fr:
        SINGLEEND_SAMPLES = [line.split(".")[0] for line in fr.readlines()]
fr.close()

# obtain EXPERIMENT MERGED
with open(config['params_preprocess']['preprocess_dir']+"Experiments_ToCombine.txt", "r") as file_a:
	FILES = [line.split("\t")[1] for line in file_a.readlines()]
file_a.close()
FILES_OUTPUT = [file.split("    ") for file in FILES]

POOLED = [item for sublist in FILES_OUTPUT for item in sublist]
POOLED_SAMPLES = [item.split(".")[0] for sublist in FILES_OUTPUT for item in sublist]

FINAL_FILES=[]
FINAL_FILES.extend(expand(config['prefix']['data']+"{single_sample}.nodup.bam", single_sample=SINGLEEND_SAMPLES))
#FINAL_FILES.extend(expand(config['prefix']['data']+"{pooled_samples}", pooled_samples=POOLED))
FINAL_FILES.extend(expand(config['prefix']['data']+"{pooled_samples}_pooled_nodup.bam", pooled_samples=POOLED_SAMPLES))

rule process_bam:
	input:	FINAL_FILES

ruleorder : remove_se_dup_filter_bam > remove_pe_dup_filter_bam > merge_pe_duplicates
rule remove_pe_dup_filter_bam:
        input:
                exe = config['prefix']['workflow_scripts']+"run_encode_filter_wrapper.sh",
                data = config['params_preprocess']['preprocess_dir']+ "unique_pairedend_h3k27ac_dhs_files.tsv"
        params:
                outdir = config['prefix']['data'],
                paired = "--paired_end",
                threads = 10
        
	output: expand(config['prefix']['data']+"{pooled_samples}", pooled_samples=POOLED)
        
	shell:
                """
                parallel -j {params.threads} bash {input.exe} ::: {input.data} ::: {params.outdir} ::: {output} ::: {params.paired}
                """

rule remove_se_dup_filter_bam:
        input:
                exe = config['prefix']['workflow_scripts']+"run_encode_filter_wrapper.sh",
                data = config['params_preprocess']['preprocess_dir']+"unique_singleend_h3k27ac_dhs_files.tsv"
	params:
                outdir = config['prefix']['data'],
                threads = 10
        
	output: expand(config['prefix']['data']+"{singlese_samples}.nodup.bam", singlese_samples=SINGLEEND_SAMPLES)
        shell:
                """
                paralle -j {params.threads} bash {input.exe} ::: {input.data} ::: {params.outdir} ::: {params.outdir}

                """

rule merge_pe_duplicates:
        input:
                exe = config['prefix']['workflow_scripts']+"merge_dup.sh",
                datafile = config['params_preprocess']['preprocess_dir']+"Experiments_ToCombine.txt",
                indir = config['prefix']['data'],
		data = expand(config['prefix']['data']+"{pooled_samples}", pooled_samples=POOLED)
	

        output: expand(config['prefix']['data']+"{pooled_samples}_pooled_nodup.bam", pooled_samples=POOLED_SAMPLES)
        
	shell:
                """
                bash {input.exe} {input.indir} {input.indir} {input.datafile}
                """ 

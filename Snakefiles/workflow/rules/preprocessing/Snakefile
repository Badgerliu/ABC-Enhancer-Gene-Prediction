import yaml 
endtype=["pairedend", "singleend"]
ASSAYS=["DHS", "H3K27ac", "ATAC"]

config = yaml.load(open('../../envs/wd.yaml'))

# obtain PAIREDEND SAMPLES
with open(config['working_dir']+config['params_preprocess']['preprocess_dir']+"unique_pairedend_h3k27ac_dhs_files.tsv", "r") as f:
        PAIREDEND_SAMPLES = [line for line in f.readlines()]
f.close()
#print(PAIREDEND_SAMPLES)

# obtain SINGLEEND SAMPLES
with open(config['working_dir']+config['params_preprocess']['preprocess_dir']+"unique_singleend_h3k27ac_dhs_files.tsv", "r") as fr:
        SINGLEEND_SAMPLES = [line for line in fr.readlines()]
fr.close()

# obtain EXPERIMENT MERGED
with open(config['working_dir']+config['params_preprocess']['preprocess_dir']+"Experiments_ToCombine.txt", "r") as file_a:
	FILES = [line.split("\t")[1] for line in file_a.readlines()]
file_a.close()
FILES_OUTPUT = [file.split("    ") for file in FILES]

POOLED = [item for sublist in FILES_OUTPUT for item in sublist]
POOLED_SAMPLES = set([item.split(".")[0] for sublist in FILES_OUTPUT for item in sublist])
print(POOLED_SAMPLES)

FINAL_FILES=[]
#FINAL_FILES.extend(expand(config['output_data_dir']+"{single_sample}.nodup.bam", single_sample=SINGLEEND_SAMPLES))
FINAL_FILES.extend(expand(config['output_data_dir']+"{pooled_samples}.nodup.bam", pooled_samples=PAIREDEND_SAMPLES))

FINAL_FILES.extend(expand(config['output_data_dir']+"{pooled_samples}_pooled.nodup.bam", pooled_samples=POOLED_SAMPLES))

rule process_bam:
	input:	FINAL_FILES

ruleorder : remove_se_dup_filter_bam > remove_pe_dup_filter_bam > merge_pe_duplicates

rule remove_pe_dup_filter_bam:
        input:
                exe = config['working_dir']+config['prefix']['workflow_scripts']+"encode_task_filter.py",
                data = config['working_dir']+config['params_preprocess']['preprocess_dir']+ "unique_pairedend_h3k27ac_dhs_files.tsv"
        params:
                outdir = config['output_data_dir'],
                paired = "--paired-end",
		pool = config['pooling_param'],
		threads = config['threads']
        
	output: expand(config['output_data_dir']+"{pooled_samples}.nodup.bam", pooled_samples=PAIREDEND_SAMPLES)
        
	shell:
                """
                python {input.exe} --input_file {input.data} {params.paired} --out_dir {params.outdir} --threads {params.threads} {params.pool} 
                """

#rule remove_se_dup_filter_bam:
#        input:
#                exe = config['working_dir']+config['prefix']['workflow_scripts']+"encode_task_filter.py",
#                data = config['working_dir']+config['params_preprocess']['preprocess_dir']+"unique_singleend_h3k27ac_dhs_files.tsv"
#	params:
#                outdir = config['output_data_dir'], 
#		pool = config['pooling_param'],
#		threads = config['threads']
#        
#	output: expand(config['output_data_dir']+"{singlese_samples}.nodup.bam", singlese_samples=SINGLEEND_SAMPLES)
#        
#	shell:
#                """
#		python {input.exe} --input_file {input.data} --out_dir {params.outdir} --threads {params.threads}
#{params.pool}
#                """

rule merge_pe_duplicates:
        input:
                exe = config['working_dir']+config['prefix']['workflow_scripts']+"merge_dup.sh",
                datafile = config['working_dir']+config['params_preprocess']['preprocess_dir']+"Experiments_ToCombine.txt",
                indir = config['output_data_dir'],
#		data = expand(config['output_data_dir']+"{pooled_samples}", pooled_samples=POOLED),

	params:	threads = config['threads']

        output: expand(config['output_data_dir']+"{pooled_samples}_pooled.nodup.bam", pooled_samples=POOLED_SAMPLES)
        
	shell:
                """
                bash {input.exe} {input.indir} {input.indir} {input.datafile} {params.threads}
                """ 
